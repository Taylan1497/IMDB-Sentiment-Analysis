{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf6bf3e",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis with Logistic Regression, k-Nearest Neighbor and Support Vector Machine Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cb3dd",
   "metadata": {},
   "source": [
    "In this project, first the dataset of the Internet Movie Database (IMDb) movie reviews will be studied on. Dataset contains huge number of reviews (approximately 50000) those are both positive and negative reviews. The negative reviews have a score lower than rating point 4 out of 10, and positive reviews have a score higher than 7. The main goal of the project is to classify whether a review is positive or negative with using at least two different architecture from the following set: Suppor Vector Machine, Logistic Regression and Naive Bayes.\n",
    "\n",
    "#### Dataset to be used :\n",
    "\n",
    "https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews/data\n",
    "\n",
    "#### References \n",
    "\n",
    "Dey, L., Chakraborty, S., Biswas, A., Bose, B., and Tiwari, S. (2016). Sentiment analysis of\n",
    "review datasets using na ̈ıve bayes‘ and k-nn classifier. International Journal of Information\n",
    "Engineering and Electronic Business, 8:54–62.\n",
    "\n",
    "Ramadhan, N. G. and Ramadhan, T. I. (2022). Analysis sentiment based on imdb aspects from\n",
    "movie reviews using svm. Sinkron : jurnal dan penelitian teknik informatika, 7(1):39–45.\n",
    "Ramadhan and Ramadhan (2022) Dey et al. (2016)\n",
    "1\n",
    "\n",
    "**Note: Other references of code and analysis will be given within the implemaentaition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d6b23",
   "metadata": {},
   "source": [
    "### OUTLINE ###\n",
    "\n",
    "#### PART 1\n",
    "\n",
    " * Part 1 contains data pre-process.\n",
    " * It will show how and why we prepare data for the model implemantaion.\n",
    "\n",
    "#### PART 2 \n",
    " \n",
    " * Part 2 contains model implementations using Scikit Learn library and a little work for KNN parameter optmization.\n",
    "\n",
    "#### PART 3 \n",
    "\n",
    " * Part 3 contains model implementations and results with Dimensionality Reduction Tecniques and Hyperparameter Optimization\n",
    " \n",
    "#### PART 4 \n",
    "\n",
    " * Part 4 contains hand-coded model implementations to the data prepared with Dimensionality Reduction and the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a6a52",
   "metadata": {},
   "source": [
    "### PART 1: Data Pre-Process\n",
    "\n",
    "* Data preprocessing is important to see what kind of data we have to deal with and for preparing those data to be implemented into the models. Our data is a text-based data\n",
    "\n",
    "* The first part of data pre-process aims at reducing the complexity by manipulating our text-based data, which enables to increase its generalization capabilities and so accuracy.\n",
    "\n",
    "* Main steps of text manipulations are Contractions Expansion, Removing HTML Tags, Lower Case Conversion, Removing Stopwords, Removing Numbers, Removing Frequent Words, Word Tokenization, Lemmatization, Removing Punctuation.\n",
    "\n",
    "* The other part of the data preprocess contains the vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e22f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2a8302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "data[\"sentiment\"]=data[\"sentiment\"].map({\"positive\":1,\"negative\":0})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2b52f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "#data.shape, data.dtypes,data.isnull().any()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec46a0",
   "metadata": {},
   "source": [
    "The column ‘review’ contains 50,000 non-null entries with a datatype object. The column ‘sentiment’ has 50,000 non-null entries with a datatype int64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c6bcb",
   "metadata": {},
   "source": [
    "The ‘sentiment’ column contains class sentiment for each review. If it is 0, then it is a negative review. If the it is 1, then it is a positive review. Lets see the number of occurrences of each and plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc110311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25000\n",
       "1    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.sentiment.value_counts().plot(kind=\"bar\")\n",
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ffb83f",
   "metadata": {},
   "source": [
    "### Text Manipulation\n",
    "Text preprocessing is an important part of any problem as we have, Natural Language Problems. I create many function for text manipulation to reduce complexity as said above.The operations are explained below:\n",
    "\n",
    "* **Contractions Expansion:** This operation changes some contracted word to the open form. Such as: \"aren't\" → 'are not'. This is required for the other operations going well.\n",
    "* **Removing HTML Tags:** This removes HTML tags like < div >< /div > and < br/ > from the text.\n",
    "* **Word Tokenization:** This turns the large text into separate tokens, basically single words.\n",
    "* **Lower Case Conversion:** This changes all the upper case characters into lower case. Upper and lower case shouldnt make any difference for our model, “A” and “a” should be same thing for our model.\n",
    "* **Removing Numbers:** This removes all the numbers in the text. Numbers do not play a significant role for the analysis of movie reviews.\n",
    "* **Removing Stopwords:** This removes english language stopwords like ‘me’, ‘our’, ‘where’, ‘what’.. etc. from the text.\n",
    "* **Removing Frequent Words:** This removes the most frequently occurring words in data, because frequent words don’t provide any significant difference among the inputs, like the word “movie”.\n",
    "* **Lemmatization:** This enables using the base forms of words. For example, “writing” → “write” and “cats” → “cat”.\n",
    "* **Removing Punctuation:** This removes all the punctuation like !”\\’(), from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25a5b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One of the other reviewers has mentioned that after watching just 1 Oz episode you will be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows would not dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ does not mess around. The first episode I ever saw struck me as so nasty it was surreal, I could not say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards whofll be sold out for a nickel, inmates whofll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contractions Expansion\n",
    "import contractions\n",
    "contractions_dict = contractions.contractions_dict\n",
    "#contractions_dict\n",
    "def contraction_expansion(x):\n",
    "    if type(x) is str:\n",
    "        for key in contractions_dict:\n",
    "            value = contractions_dict[key]\n",
    "            x = x.replace(key, value)\n",
    "        return x\n",
    "    else:\n",
    "        return x\n",
    "data[\"review\"]=data[\"review\"].apply(lambda x : contraction_expansion(x))\n",
    "data.iloc[0][\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3392c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove html tags\n",
    "import re\n",
    "def remove_html(element):\n",
    "    html_removed = re.sub(re.compile('<.*?>'),'',element)\n",
    "    return html_removed\n",
    "data[\"review\"]=data[\"review\"].apply(lambda x : remove_html(x))\n",
    "#data.iloc[0][\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "692f0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuation\n",
    "from string import punctuation\n",
    "def remove_punct(element):\n",
    "    element_without_punct = [i for i in element if not i in punctuation]\n",
    "    element_without_punct=''.join(element_without_punct)\n",
    "    return element_without_punct\n",
    "data[\"review\"]=data[\"review\"].apply(lambda x:remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0addd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/taylan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "data[\"review\"]=data[\"review\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb3d9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Case Transformation\n",
    "def lower_case(element):\n",
    "    lower_case_element = element.lower()\n",
    "    return lower_case_element\n",
    "data[\"review\"]=data[\"review\"].apply(lambda x : [lower_case(i) for i in x])\n",
    "#data.iloc[0][\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692b27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Numbers\n",
    "def number_remove(element):\n",
    "    element_without_numbers = [i for i in element if not i.isdigit()]\n",
    "    #Yes_number = element.isdigit() # if it it is a number, it returns True\n",
    "    return element_without_numbers\n",
    "data[\"review\"]=data[\"review\"].apply(lambda x:number_remove(x))\n",
    "#data.iloc[0][\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "120abbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stops=stopwords.words(\"english\")\n",
    "def remove_stop(element):\n",
    "    element_without_stop =[i for i in element if i not in stops]\n",
    "    return element_without_stop\n",
    "data[\"review\"]=data[\"review\"].apply(lambda x:remove_stop(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459d0ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'reviewers',\n",
       " 'mentioned',\n",
       " 'watching',\n",
       " 'oz',\n",
       " 'episode',\n",
       " 'hooked',\n",
       " 'right',\n",
       " 'exactly',\n",
       " 'happened',\n",
       " 'methe',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'struck',\n",
       " 'oz',\n",
       " 'brutality',\n",
       " 'unflinching',\n",
       " 'scenes',\n",
       " 'violence',\n",
       " 'set',\n",
       " 'right',\n",
       " 'word',\n",
       " 'go',\n",
       " 'trust',\n",
       " 'show',\n",
       " 'faint',\n",
       " 'hearted',\n",
       " 'timid',\n",
       " 'show',\n",
       " 'pulls',\n",
       " 'punches',\n",
       " 'regards',\n",
       " 'drugs',\n",
       " 'sex',\n",
       " 'violence',\n",
       " 'hardcore',\n",
       " 'classic',\n",
       " 'use',\n",
       " 'wordit',\n",
       " 'called',\n",
       " 'oz',\n",
       " 'nickname',\n",
       " 'given',\n",
       " 'oswald',\n",
       " 'maximum',\n",
       " 'security',\n",
       " 'state',\n",
       " 'penitentary',\n",
       " 'focuses',\n",
       " 'mainly',\n",
       " 'emerald',\n",
       " 'city',\n",
       " 'experimental',\n",
       " 'section',\n",
       " 'prison',\n",
       " 'cells',\n",
       " 'glass',\n",
       " 'fronts',\n",
       " 'face',\n",
       " 'inwards',\n",
       " 'privacy',\n",
       " 'high',\n",
       " 'agenda',\n",
       " 'em',\n",
       " 'city',\n",
       " 'home',\n",
       " 'manyaryans',\n",
       " 'muslims',\n",
       " 'gangstas',\n",
       " 'latinos',\n",
       " 'christians',\n",
       " 'italians',\n",
       " 'irish',\n",
       " 'moreso',\n",
       " 'scuffles',\n",
       " 'death',\n",
       " 'stares',\n",
       " 'dodgy',\n",
       " 'dealings',\n",
       " 'shady',\n",
       " 'agreements',\n",
       " 'never',\n",
       " 'far',\n",
       " 'awayi',\n",
       " 'would',\n",
       " 'say',\n",
       " 'main',\n",
       " 'appeal',\n",
       " 'show',\n",
       " 'due',\n",
       " 'fact',\n",
       " 'goes',\n",
       " 'shows',\n",
       " 'would',\n",
       " 'dare',\n",
       " 'forget',\n",
       " 'pretty',\n",
       " 'pictures',\n",
       " 'painted',\n",
       " 'mainstream',\n",
       " 'audiences',\n",
       " 'forget',\n",
       " 'charm',\n",
       " 'forget',\n",
       " 'romanceoz',\n",
       " 'mess',\n",
       " 'around',\n",
       " 'first',\n",
       " 'episode',\n",
       " 'ever',\n",
       " 'saw',\n",
       " 'struck',\n",
       " 'nasty',\n",
       " 'surreal',\n",
       " 'could',\n",
       " 'say',\n",
       " 'ready',\n",
       " 'watched',\n",
       " 'developed',\n",
       " 'taste',\n",
       " 'oz',\n",
       " 'got',\n",
       " 'accustomed',\n",
       " 'high',\n",
       " 'levels',\n",
       " 'graphic',\n",
       " 'violence',\n",
       " 'violence',\n",
       " 'injustice',\n",
       " 'crooked',\n",
       " 'guards',\n",
       " 'whofll',\n",
       " 'sold',\n",
       " 'nickel',\n",
       " 'inmates',\n",
       " 'whofll',\n",
       " 'kill',\n",
       " 'order',\n",
       " 'get',\n",
       " 'away',\n",
       " 'well',\n",
       " 'mannered',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'inmates',\n",
       " 'turned',\n",
       " 'prison',\n",
       " 'bitches',\n",
       " 'due',\n",
       " 'lack',\n",
       " 'street',\n",
       " 'skills',\n",
       " 'prison',\n",
       " 'experience',\n",
       " 'watching',\n",
       " 'oz',\n",
       " 'may',\n",
       " 'become',\n",
       " 'comfortable',\n",
       " 'uncomfortable',\n",
       " 'viewingthats',\n",
       " 'get',\n",
       " 'touch',\n",
       " 'darker',\n",
       " 'side']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Frequent Words \n",
    "temp = data['review'].apply(lambda words: \" \".join(words))\n",
    "freq = pd.Series(temp).value_counts()[:10]\n",
    "data['review'] = data['review'].apply(lambda words: [x for x in words if x not in freq.keys()])\n",
    "data.iloc[0][\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c34df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/taylan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmetization \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmetize(element):\n",
    "    \n",
    "    lemmatized_output = [lemmatizer.lemmatize(w) for w in element]\n",
    "    lemmatized_output = \" \".join(lemmatized_output)\n",
    "    \n",
    "    return lemmatized_output\n",
    "data[\"review\"]=data[\"review\"].apply(lambda x: lemmetize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0069c",
   "metadata": {},
   "source": [
    "### Vectorization \n",
    "\n",
    "There is no model that can understand the text-based data, so we need to transform our text-based data into the for of that the models can understand. There are two main approches for this, which are Count Vectorizer and Term Frequency Inverse Document Frequency(TF-IDF) Vectorizer. Count Vectorizer gives number of frequency with respect to index of vocabulary. On the other hand, Tf-Idf consider overall documents of weight of words. For more details you can visit page : https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a \n",
    "\n",
    "Count vectorizer has some disadvantages as follows: inability in identifying more important and less important words for the analysis, consideration of words that are found mostly in a corpus as the most statistically significant word. On the other side, Tf-Idf is a statistic that is based on the frequency of a word in the corpus. It also provides a numerical representation of how important a word is for statistical analysis. TF-IDF is better than Count Vectorizers since it considers both the frequency of words present and  the importance of the words in the text which provides us working with less input dimension and important words.\n",
    "\n",
    "Also Dimensionality Reductions tecniques can be applied easier for Tf-Idf, so the chosen one is it. All tools for vectorization, model implementation and measuring model performance are got from scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a94de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All tools for vectorization, model implementation and measuring model performance here:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "885f1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization and Train-Test Split\n",
    "X = data['review']\n",
    "y = data['sentiment']\n",
    "tfidf = TfidfVectorizer()#max_features=300)#max_features=10) --> No Dimensionality Reduction for now.\n",
    "X = tfidf.fit_transform(X)\n",
    "#y=tfidf.fit_transform(y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3da9c",
   "metadata": {},
   "source": [
    "**We see very high dimensional case!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "923f1414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 208350), (10000, 208350), (40000,), (10000,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb5337c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_frame = pd.DataFrame(x_train.toarray())\n",
    "#x_train_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eddad69",
   "metadata": {},
   "source": [
    "## PART 2 : Model Implementation using Scikit-Learn Library\n",
    "\n",
    "#### 1) Logistic Regression\n",
    "\n",
    "* dual: Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.\n",
    "\n",
    "* max_iter: default=100 Maximum number of iterations of the optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5192dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89175, 0.89125, 0.894  , 0.88925, 0.89025, 0.89475, 0.883  ,\n",
       "       0.88775, 0.8865 , 0.8905 ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "scores = cross_val_score(logistic_model, x_train, y_train, cv=10, n_jobs=4)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df2ffb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.894, 0.932475)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.fit(x_train, y_train)\n",
    "logistic_model.score(x_test,y_test),logistic_model.score(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0043b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4367,  594],\n",
       "       [ 466, 4573]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logistic_model.predict(x_test)\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a8cda",
   "metadata": {},
   "source": [
    "#### 2) K-Neares Neighbour\n",
    "\n",
    "* n_neighbours : determine number of neighbours, k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3085f74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8f3a362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3434, 1527],\n",
       "       [ 848, 4191]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=knn.predict(x_test)\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2ba0d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7625, 0.882975)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(x_test,y_test), knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd121aa7",
   "metadata": {},
   "source": [
    "**There is an overfitting problem here !!**\n",
    "\n",
    "**The value of k in the KNN algorithm is related to the error rate of the model. A small value of k could lead to overfitting as well as a big value of k can lead to underfitting. Also, dimesionality reduction can be help to perform better!!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a776ea94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3476, 1485],\n",
       "       [ 790, 4249]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_train,y_train)\n",
    "y_pred=knn.predict(x_test)\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34036dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7725, 0.856125)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(x_test,y_test), knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8400c86",
   "metadata": {},
   "source": [
    "**Better but not enough!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "083e1454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3806, 1155],\n",
       "       [ 938, 4101]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(x_train,y_train)\n",
    "y_pred=knn.predict(x_test)\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8431a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7907, 0.8368)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(x_test,y_test), knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e4785ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3545, 1416],\n",
       "       [ 684, 4355]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(x_train,y_train)\n",
    "y_pred=knn.predict(x_test)\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4053349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.79, 0.8233)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(x_test,y_test), knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8865c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2914fc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3773, 1188],\n",
       "       [ 921, 4118]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=12)\n",
    "knn.fit(x_train,y_train)\n",
    "y_pred=knn.predict(x_test)\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "083d6354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7891, 0.831625)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(x_test,y_test), knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d301d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "315e1731",
   "metadata": {},
   "source": [
    "#### 3) Support Vector Machine\n",
    "\n",
    "It doesnt converge for time 1 hour. It needs dimensionality reduction teqniques!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b7076c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-31-7b4b0c1d70bc>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-7b4b0c1d70bc>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    confusion_matrix(y_test,y_pred)\u001b[0m\n\u001b[0m                                   \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "''''from sklearn import svm\n",
    "\n",
    "%%time\n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1881b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73765e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "280d706d",
   "metadata": {},
   "source": [
    "# PART 3 : Dimensionality Reduction and Hyperparemeter Optimization for Models\n",
    "\n",
    "* In this part again Scikit Learn Library will be used for model implementation\n",
    "* First Dimensionality Reduction teqniques are used and then model implementation.\n",
    "* At the end, comparision with model results that created without dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258d918",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "\n",
    "* It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. (see more information here: https://en.wikipedia.org/wiki/Tf%E2%80%93idf )\n",
    "\n",
    "* max_features : The purpose of max_features is to limit the number of features (words) from the dataset for which we want to calculate the TF-IDF scores. This is done by choosing the features based on term frequency across the corpus. More info : https://www.deepwizai.com/projects/how-to-correctly-use-tf-idf-with-imbalanced-data\n",
    "\n",
    "* So max_feature paremeter for TfidfVectorizer is a teqnique for reduction od dimension, so complexity, it is a very essential dimensinality reduction tecniques like our text processing functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b59130ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X = data['review']\n",
    "y = data['sentiment']\n",
    "tfidf = TfidfVectorizer(max_features=300)\n",
    "X = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b1664b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 300), (50000,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "369e2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4a1e4",
   "metadata": {},
   "source": [
    "### Logistic Model\n",
    "\n",
    "* Before accuracy: (0.894, 0.932475) for:(test accuracy,train accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "539646ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8131, 0.816875)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "logistic_model.score(x_test,y_test),logistic_model.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1008fc94",
   "metadata": {},
   "source": [
    "* Current  accuracy : (0.8807, 0.89935) overfittin is better. Since The feature number decreases it didnt give better test result. with the increasing number of max_feature paremeter it can give better result than the before test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd19c3",
   "metadata": {},
   "source": [
    "### KNN Model \n",
    "\n",
    "* Before accuracies with some spesific k and the form (test accuracy,train accuracy)\n",
    "  * k=3 (0.7625, 0.882975)\n",
    "  * k=5 (0.7725, 0.856125)\n",
    "  * k=10 (0.7907, 0.8368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0053da72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.668, 0.931225)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(x_train,y_train)\n",
    "#y_pred=knn.predict(x_test)\n",
    "#confusion_matrix(y_test,y_pred)\n",
    "knn.score(x_test,y_test), knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9f80089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6927, 0.835)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_train,y_train)\n",
    "#y_pred=knn.predict(x_test)\n",
    "#confusion_matrix(y_test,y_pred)\n",
    "knn.score(x_test,y_test), knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f020773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7207, 0.79205)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(x_train,y_train)\n",
    "#y_pred=knn.predict(x_test)\n",
    "#confusion_matrix(y_test,y_pred)\n",
    "knn.score(x_test,y_test), knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a49f8",
   "metadata": {},
   "source": [
    "* After accuracies with some spesific k and the form (test accuracy,train accuracy)\n",
    "  * k=3 (0.7293, 0.85995)\n",
    "  * k=5 (0.7358, 0.830275)\n",
    "  * k=10 (0.7614, 0.81395)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17963f0",
   "metadata": {},
   "source": [
    "### SVM Model \n",
    "\n",
    "* Before: It didint give a result in reasonable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85a75022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.927975, 0.8185)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "#y_pred = clf.predict(x_test)\n",
    "#confusion_matrix(y_test,y_pred)\n",
    "clf.score(x_train,y_train),clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec757100",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test,y_pred)*100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87c83a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.85"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a08fd",
   "metadata": {},
   "source": [
    "* After : It gave result but, again train score higher than test score , (0.885,0.976), form of (test accuracy,train accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be0b4d",
   "metadata": {},
   "source": [
    "### Principal Component Analysis and Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b22a5f",
   "metadata": {},
   "source": [
    "TF-IDF is a scoring method that assigns a score to each word in the sentence. The score is high if the word does not commonly occur in all the reviews but occurs frequently overall. For example, the word course occurs frequently, but it occurs in almost every other review. So TF-IDF score for that word would below. On the other hand, the word poor occurs frequently and is specific to only negative reviews. So its score is higher.\n",
    "\n",
    "So for every review, we have fixed length TF-IDF vector. The length of the vector is the size of the vocabulary.\n",
    "\n",
    "How to reduce dimensionality, lets use Principial Component Analysis(PCA). What is PCA? \n",
    "\n",
    "PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points. \n",
    "\n",
    "Links:\n",
    "\n",
    "https://towardsdatascience.com/introduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57\n",
    "https://en.wikipedia.org/wiki/Principal_component_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ea9fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000) # for default Unable to allocate 77.6 GiB for an array with shape (50000, 208350) and data type float64\n",
    "reviews = list(data['review'])                  # so set max_feature \n",
    "labels = data['sentiment']\n",
    "\n",
    "tfidf_reviews = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95dbad4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the array: (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_array = tfidf_reviews.toarray()\n",
    "print(\"Shape of the array:\",tfidf_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71f9687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of zeros: 98.4765016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Percentage of zeros:\",\n",
    "      np.count_nonzero(tfidf_array==0)/(tfidf_array.shape[0]*tfidf_array.shape[1])*100)\n",
    "#only 0.078% of the array elements are non zero. Such a waste of space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4554c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "NUM_COMPONENTS = 300\n",
    "pca = PCA(NUM_COMPONENTS)\n",
    "reduced = pca.fit_transform(tfidf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55a76b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7a5f6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(reduced, labels, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77bd7ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 300)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e70dd987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.87, 0.87245)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "logistic_model.score(x_test,y_test),logistic_model.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69892713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7087, 0.7817)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(x_train,y_train)\n",
    "#y_pred=knn.predict(x_test)\n",
    "#confusion_matrix(y_test,y_pred)\n",
    "knn.score(x_test,y_test), knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5180d681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9493, 0.879)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "#y_pred = clf.predict(x_test)\n",
    "#confusion_matrix(y_test,y_pred)\n",
    "clf.score(x_train,y_train),clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d117784",
   "metadata": {},
   "source": [
    "### Very Simple way of avoiding Overfitting:Reducing Training Samples\n",
    "\n",
    "* Lets try reducing training samples, it can prevent model to learn exesively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2c586ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "reviews = data['review'].iloc[:25000]\n",
    "label = data['sentiment'].iloc[:25000]\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf_reviews = tfidf.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f168f403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the array: (25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_array = tfidf_reviews.toarray()\n",
    "print(\"Shape of the array:\",tfidf_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bffebd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "NUM_COMPONENTS = 300\n",
    "pca = PCA(NUM_COMPONENTS)\n",
    "reduced = pca.fit_transform(tfidf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82011618",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(reduced, label, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "410dee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 300), (5000, 300), (20000,))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08018325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8632, 0.87355)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "logistic_model.score(x_test,y_test),logistic_model.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a65b0a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7068, 0.78685)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(x_train,y_train)\n",
    "#y_pred=knn.predict(x_test)\n",
    "#confusion_matrix(y_test,y_pred)\n",
    "knn.score(x_test,y_test), knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8f7ad4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.865, 0.9536)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "#y_pred = clf.predict(x_test)\n",
    "#confusion_matrix(y_test,y_pred)\n",
    "clf.score(x_test,y_test), clf.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f490e9",
   "metadata": {},
   "source": [
    "### Pre-Results\n",
    "\n",
    "* 1) Without any Dimensionality Reduction, we have very high demsional feature space. Our training input x_train.shape : (40000, 208350). And the models gives results as follows, in  the order of test accuracy and train accuracy:\n",
    "\n",
    "   * Logistic Model Result :\n",
    "   \n",
    "       * (0.894, 0.932475)\n",
    "       \n",
    "   * KNN Model Results for different k values:\n",
    "   \n",
    "       * k=3 : (0.7625, 0.882975)\n",
    "       * k=5 : (0.7725, 0.856125)\n",
    "       * k=10 : (0.7907, 0.8368)\n",
    "       * k=12 : (0.7891, 0.831625)\n",
    "       * k=15 : (0.79, 0.8233)\n",
    "       \n",
    "   * SVM Model Resul :\n",
    "   \n",
    "       * It didnt give result in a reasonable time.\n",
    "       \n",
    "\n",
    "* 2) With an simple complexity reduction by directly reducing feauture number using paremeter max_feature of Tf-Idf Vectorizer as explained above. Our training input x_train.shape : (40000, 300) The model gives results as follows, in  the order of test accuracy and train accuracy:\n",
    "\n",
    "   * Logistic Model Result :\n",
    "   \n",
    "       * (0.8131, 0.816875)\n",
    "       \n",
    "   * KNN Model Results for different k values:\n",
    "   \n",
    "       * k=3 (0.7293, 0.85995)\n",
    "       * k=5 (0.7358, 0.830275)\n",
    "       * k=10 (0.7614, 0.81395)\n",
    "       \n",
    "   * SVM Model Result :\n",
    "   \n",
    "       * (0.885,0.976)\n",
    "\n",
    "\n",
    "\n",
    "* 3) Dimesion Reduction with Principal Component Analysis and Tf-Idf. Our training input x_train.shape : (40000,300)\n",
    "\n",
    "   * Logistic Model Result :\n",
    "   \n",
    "       * (0.87, 0.871975)\n",
    "       \n",
    "   * KNN Model Results for different k values:\n",
    "   \n",
    "       * k=10 (0.715, 0.78515)\n",
    "       \n",
    "   * SVM Model Result :\n",
    "   \n",
    "       * (0.8799, 0.9498)\n",
    "       \n",
    "* 4) Reducing Training Samples for avoiding Overfitting. Our training input x_train.shape : (20000, 200). It converges faster as expected.\n",
    "\n",
    "   * Logistic Model Result :\n",
    "   \n",
    "       * (0.8598, 0.87155)\n",
    "       \n",
    "   * KNN Model Results for different k values:\n",
    "   \n",
    "       * k=10 (0.707, 0.78825)\n",
    "       \n",
    "   * SVM Model Result :\n",
    "   \n",
    "       * (0.8632, 0.9531)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca2ef9",
   "metadata": {},
   "source": [
    "### Comment on Pre-Results\n",
    "\n",
    "* Dimesionality reduction leads to models perform faster, SVM converges but there is an overfitting problem even though test accuracy results are satifactory. Differen complexity reduction teqniques are performed and all gave the same overfitting result. Just for Logistic Regression model, we can say there is no overfitting.\n",
    "\n",
    "* There can be many reasons for overfitting for each model. Consideration of that will be given later as in the topic of Possible Reason of Overfitting.\n",
    "\n",
    "* As well as this, increasing k until some optimal point, it gives better result. It will be considered in hyperparameter optimization part. Hyperparemeter optimization may also reduce overfitting.\n",
    "\n",
    "* Best performance is shown by Logistic Regression, then SVM and the worst is KNN model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df1ee27",
   "metadata": {},
   "source": [
    "### OverFitting for Models \n",
    "\n",
    "* KNN https://online.stat.psu.edu/stat508/lesson/9/9.3\n",
    "\n",
    "    * https://www.baeldung.com/cs/k-nearest-neighbors\n",
    "    \n",
    "    * Smoothing. To prevent overfitting, we can smooth the decision boundary by K nearest neighbors.\n",
    "    \n",
    "    * The amount of computation can be intense when the training data is large since the distance between a new data point and every training point has to be computed and sorted.\n",
    "\n",
    "    * Feature standardization is often performed in pre-processing. Because standardization affects the distance, if one wants the features to play a similar role in determining the distance, standardization is recommended. However, whether to apply normalization is rather subjective. One has to decide on an individual basis for the problem in consideration.\n",
    "\n",
    "    * The only parameter that can adjust the complexity of KNN is the number of neighbors k. The larger k is, the smoother the classification boundary. Or we can think of the complexity of KNN as lower when k increases.\n",
    "    \n",
    "    * It's main disadvantages are that it is quite computationally inefficient and its difficult to pick the “correct” value of K. \n",
    "    \n",
    "    * And also very high k can lead underfitting. Thats why it has an optimal point.\n",
    "    \n",
    "    \n",
    "* Logistic Regression http://eointravers.com/post/logistic-overfit/\n",
    "  \n",
    "  * Sometimes, you end up with situations where the model wants to predict y=1  or y=0 . This happens when it’s possible to draw a straight line through your data so that every y=1 on one side of the line, and 0 on the other. This is called perfect separation.\n",
    "\n",
    "  *  To do this, it must set the regression weights,  as large as possible. In theory, the model could fit the data if you could set wights as large as possible, but your software can’t do that. Instead, it iteratively tries higher and higher values until it reaches values that are too large for your computer to store.\n",
    "  \n",
    "  * The more predictors you have (the higher the dimensionality), the more likely it is that it will be possible to perfectly separate the two sets of values. As a result, overfitting becomes more of an issue when you have many predictors.\n",
    "  \n",
    "  \n",
    "* SVM Model https://stats.stackexchange.com/questions/35276/svm-overfitting-curse-of-dimensionality\n",
    "\n",
    "  *  In practice, the reason that SVMs tend to be resistant to over-fitting, even in cases where the number of attributes is greater than the number of observations, is that it uses regularization. They key to avoiding over-fitting lies in careful tuning of the regularization parameter, C, and in the case of non-linear SVMs, careful choice of kernel and tuning of the kernel parameters.\n",
    "\n",
    "  * The SVM is an approximate implementation of a bound on the generalization error, that depends on the margin (essentially the distance from the decision boundary to the nearest pattern from each class), but is independent of the dimensionality of the feature space (which is why using the kernel trick to map the data into a very high dimensional space isn't such a bad idea as it might seem). So in principle SVMs should be highly resistant to over-fitting, but in practice this depends on the careful choice of C and the kernel parameters. Sadly, over-fitting can also occur quite easily when tuning the hyper-parameters as well.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b92f69",
   "metadata": {},
   "source": [
    "## HyperParameter Optimizations\n",
    "\n",
    "Hyperparameter tuning (or hyperparameter optimization) is the process of determining the right combination of hyperparameters that maximizes the model performance. \n",
    "\n",
    "* Logistic Model\n",
    "\n",
    "    * Regularization, Daul Formulation effect on result is performed.\n",
    "        \n",
    "    * max_iter and C values are performed.\n",
    "    \n",
    "* KNN Model \n",
    "    \n",
    "    * k value optimization is performed.\n",
    "    \n",
    "* SVM Model \n",
    "\n",
    "    * C parameter and kernel paremeters optmization is performed.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcea6544",
   "metadata": {},
   "source": [
    "### Logistic Model Regularization and Dual Formulation Effect on Overfitting, \n",
    "\n",
    "* Regulurization is importon to prune overfitting, as default Scikit Learn Logistic model has regulurization Lets see what happens we add regulurization and dont add regulurization\n",
    "\n",
    "* penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "    None: no penalty is added;\n",
    "\n",
    "    'l2': add a L2 penalty term and it is the default choice;\n",
    "\n",
    "    'l1': add a L1 penalty term;\n",
    "\n",
    "    'elasticnet': both L1 and L2 penalty terms are added.\n",
    "\n",
    "* Cfloat, default=1.0  Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "\n",
    "* dualbool, default=False Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.\n",
    "\n",
    "* solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’ Algorithm to use in the optimization problem. Default is ‘lbfgs’. To choose a solver, you might want to consider the following aspects: For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones; For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ and is limited to one-versus-rest schemes. ‘newton-cholesky’ is a good choice for n_samples >> n_features, especially with one-hot encoded categorical features with rare categories. Note that it is limited to binary classification and the one-versus-rest reduction for multiclass classification. Be aware that the memory usage of this solver has a quadratic dependency on n_features because it explicitly computes the Hessian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4bb041a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.862, 0.8754), (0.862, 0.87545), (0.862, 0.87545), (0.8658, 0.87315))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logistic_model = LogisticRegression(max_iter=200, C=1000) # default l2 ridge regurulization\n",
    "logistic_model.fit(x_train, y_train)\n",
    "first = logistic_model.score(x_test,y_test),logistic_model.score(x_train,y_train)\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=200, C=1000000) #make weaker penalty term\n",
    "logistic_model.fit(x_train, y_train)\n",
    "second = logistic_model.score(x_test,y_test),logistic_model.score(x_train,y_train)\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=200, penalty=\"none\") # no penalty term\n",
    "logistic_model.fit(x_train, y_train)\n",
    "third = logistic_model.score(x_test,y_test),logistic_model.score(x_train,y_train)\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=200, penalty=\"l1\",solver=\"liblinear\") # with L1 regularization\n",
    "logistic_model.fit(x_train, y_train)\n",
    "fourth = logistic_model.score(x_test,y_test),logistic_model.score(x_train,y_train)\n",
    "\n",
    "first,second,third,fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7edf3149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8636, 0.87355)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With Dual Formulation\n",
    "logistic_model = LogisticRegression(max_iter=200, penalty=\"l2\",solver=\"liblinear\",dual=True)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "logistic_model.score(x_test,y_test),logistic_model.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2aa6c4",
   "metadata": {},
   "source": [
    "**We can say regularization and dual formulation have almost zero effect on the results for our case, but it needs mode study on it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d30df4",
   "metadata": {},
   "source": [
    "### To optmize time for Hyperparameter optimization, lets create train and test data for less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "973a4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X = data['review'].iloc[:25000]\n",
    "y = data['sentiment'].iloc[:25000]\n",
    "tfidf = TfidfVectorizer(max_features=500)\n",
    "X = tfidf.fit_transform(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f14b8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_max_iter(upper,lower):\n",
    "    test_score=[]\n",
    "    train_score=[]\n",
    "    iterations=[]\n",
    "    optimum_iter=0\n",
    "    key_score=0\n",
    "    for i in range(upper,lower):\n",
    "        if i%20==0:\n",
    "            logistic_model = LogisticRegression(max_iter=i)\n",
    "            logistic_model.fit(x_train, y_train)\n",
    "            test_accuracy=logistic_model.score(x_test,y_test)\n",
    "            train_accuracy=logistic_model.score(x_train,y_train)\n",
    "            test_score.append(logistic_model.score(x_test,y_test))\n",
    "            train_score.append(logistic_model.score(x_train,y_train))\n",
    "            iterations.append(i)\n",
    "            if test_accuracy>key_score:\n",
    "                optimum_iter=i\n",
    "                key_score=test_accuracy\n",
    "        else:\n",
    "            continue\n",
    "    train_test_frame = pd.DataFrame({\"Test Scores\": test_score,\"Train Scores\":train_score})\n",
    "    #train_test_frame.plot()\n",
    "    Main_frame = pd.DataFrame({\"Iterations\":iterations,\"Test Scores\": test_score,\"Train Scores\":train_score})\n",
    "    plt.plot(iterations,test_score,label=\"Test Scores\")\n",
    "    plt.plot(iterations,train_score,label=\"Train Scores\")\n",
    "    plt.xlabel=\"Iterations\"\n",
    "    plt.ylabel=\"Accuracies\"\n",
    "    print(\"Optimum Score:\",key_score,\"The Iteration Number:\",optimum_iter)\n",
    "    return Main_frame,key_score,optimum_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "39031dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 500)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f60de98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum Score: 0.8346 The Iteration Number: 60\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Main_frame,key_score,optimum_iter = optimize_max_iter(50,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "61f0a840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iterations</th>\n",
       "      <th>Test Scores</th>\n",
       "      <th>Train Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>160</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>180</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>220</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>240</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>260</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>280</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.8472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iterations  Test Scores  Train Scores\n",
       "0           60       0.8346        0.8472\n",
       "1           80       0.8346        0.8472\n",
       "2          100       0.8346        0.8472\n",
       "3          120       0.8346        0.8472\n",
       "4          140       0.8346        0.8472\n",
       "5          160       0.8346        0.8472\n",
       "6          180       0.8346        0.8472\n",
       "7          200       0.8346        0.8472\n",
       "8          220       0.8346        0.8472\n",
       "9          240       0.8346        0.8472\n",
       "10         260       0.8346        0.8472\n",
       "11         280       0.8346        0.8472"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Iter_values_frame=Main_frame.sort_values(ascending=True,by=\"Test Scores\")\n",
    "Iter_values_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6d9cacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_c_values(C_list):\n",
    "    test_score=[]\n",
    "    train_score=[]\n",
    "    C_Values=C_list\n",
    "    optimum_C=0\n",
    "    key_score=0\n",
    "    for i in C_list:\n",
    "        #key=i*1000\n",
    "        #if key%10==0:\n",
    "        logistic_model = LogisticRegression(max_iter=200, C=i)\n",
    "        logistic_model.fit(x_train, y_train)\n",
    "        test_accuracy=logistic_model.score(x_test,y_test)\n",
    "        train_accuracy=logistic_model.score(x_train,y_train)\n",
    "        test_score.append(logistic_model.score(x_test,y_test))\n",
    "        train_score.append(logistic_model.score(x_train,y_train))\n",
    "        #iterations.append(i)\n",
    "        if test_accuracy>key_score:\n",
    "            optimum_C=i\n",
    "            key_score=test_accuracy\n",
    "        else:\n",
    "            continue\n",
    "    train_test_frame = pd.DataFrame({\"Test Scores\": test_score,\"Train Scores\":train_score})\n",
    "    #train_test_frame.plot()\n",
    "    Main_frame = pd.DataFrame({\"C_Values\":C_Values,\"Test Scores\": test_score,\"Train Scores\":train_score})\n",
    "    plt.plot(C_Values,test_score, label=\"Test Scores\")\n",
    "    plt.plot(C_Values,train_score, label=\"Train Scores\")\n",
    "    #plt.xlabel(\"C Values\")\n",
    "    #plt.ylabel(\"Accuracies\")\n",
    "    #train_test_frame.plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"Optimum Score:\",key_score,\"The C Number:\",optimum_C)\n",
    "    return Main_frame,key_score,optimum_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "64f9a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [100000,1000,100, 10, 1.0, 0.1, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67c4cb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum Score: 0.8354 The C Number: 100000\n"
     ]
    }
   ],
   "source": [
    "Main_frame,key_score,optimum_C = optimize_c_values(c_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "acdd4a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_Values</th>\n",
       "      <th>Test Scores</th>\n",
       "      <th>Train Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000.00</td>\n",
       "      <td>0.8354</td>\n",
       "      <td>0.84935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.8354</td>\n",
       "      <td>0.84930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.00</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.84935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.8348</td>\n",
       "      <td>0.84945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.8346</td>\n",
       "      <td>0.84720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8332</td>\n",
       "      <td>0.83590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.81180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C_Values  Test Scores  Train Scores\n",
       "0  100000.00       0.8354       0.84935\n",
       "1    1000.00       0.8354       0.84930\n",
       "2     100.00       0.8350       0.84935\n",
       "3      10.00       0.8348       0.84945\n",
       "4       1.00       0.8346       0.84720\n",
       "5       0.10       0.8332       0.83590\n",
       "6       0.01       0.8102       0.81180"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_values_frame=Main_frame.sort_values(ascending=False,by=\"Test Scores\")\n",
    "C_values_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0bb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec718535",
   "metadata": {},
   "source": [
    "### KNN k Parameter Optmization\n",
    "\n",
    "* To work faster, less data is used.\n",
    "\n",
    "* Choosing a large k value leads to high bias by classifying everything as the more probable class. As a result, we have a smooth decision boundary between the classes, with less emphasis on individual points.\n",
    "\n",
    "* Conversely, small k values cause high variance and an unstable output. Minor changes in the training set cause large changes in classification boundaries. The effect is more visible on intersecting classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9b4a4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X = data['review'].iloc[:5000]\n",
    "y = data['sentiment'].iloc[:5000]\n",
    "tfidf = TfidfVectorizer(max_features=500)\n",
    "X = tfidf.fit_transform(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3c6fdd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_k_values(upper,lower):\n",
    "    test_score=[]\n",
    "    train_score=[]\n",
    "    k_values=[]\n",
    "    optimum_C=0\n",
    "    key_score=0\n",
    "    for i in range(upper,lower):\n",
    "        key=i\n",
    "        if key%1==0:\n",
    "            knn_model=KNeighborsClassifier(n_neighbors=i)\n",
    "            knn_model.fit(x_train,y_train)\n",
    "            test_accuracy=knn_model.score(x_test,y_test)\n",
    "            train_accuracy=knn_model.score(x_train,y_train)\n",
    "            test_score.append(knn_model.score(x_test,y_test))\n",
    "            train_score.append(knn_model.score(x_train,y_train))\n",
    "            k_values.append(i)\n",
    "            if test_accuracy>key_score:\n",
    "                optimum_k=i\n",
    "                key_score=test_accuracy\n",
    "        else:\n",
    "            continue\n",
    "    train_test_frame = pd.DataFrame({\"Test Scores\": test_score,\"Train Scores\":train_score})\n",
    "    #train_test_frame.plot()\n",
    "    Main_frame = pd.DataFrame({\"k_Values\":k_values,\"Test Scores\": test_score,\"Train Scores\":train_score})\n",
    "    plt.plot(k_values,test_score, label=\"Test Scores\")\n",
    "    plt.plot(k_values,train_score, label=\"Train Scores\")\n",
    "    #plt.xlabel(\"C Values\")\n",
    "    #plt.ylabel(\"Accuracies\")\n",
    "    #train_test_frame.plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"Optimum Score:\",key_score,\"The k Number:\",optimum_k)\n",
    "    return Main_frame,key_score,optimum_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "545973ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum Score: 0.766 The k Number: 25\n"
     ]
    }
   ],
   "source": [
    "Main_frame,key_score,optimum_k = optimize_k_values(2,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ecdd020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k_Values</th>\n",
       "      <th>Test Scores</th>\n",
       "      <th>Train Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.78800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.78600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.79225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.78625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.78075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.78750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.78675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.78100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.78300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.78625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.78900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.78450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.78075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.78650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.78550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.78575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.78150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.78375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.78675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.78600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.77950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.78875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.78375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.78650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.79225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.78000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.78750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.78875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.79025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.78800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.79675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.80575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.78825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.79775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.82350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.80975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.85225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.82225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k_Values  Test Scores  Train Scores\n",
       "23        25        0.766       0.78800\n",
       "29        31        0.761       0.78600\n",
       "25        27        0.760       0.79225\n",
       "22        24        0.760       0.78625\n",
       "34        36        0.760       0.78075\n",
       "24        26        0.760       0.78750\n",
       "26        28        0.759       0.78675\n",
       "32        34        0.758       0.78100\n",
       "31        33        0.758       0.78300\n",
       "20        22        0.758       0.78625\n",
       "27        29        0.757       0.78900\n",
       "35        37        0.756       0.78450\n",
       "33        35        0.756       0.78075\n",
       "21        23        0.755       0.78650\n",
       "28        30        0.755       0.78550\n",
       "30        32        0.754       0.78575\n",
       "36        38        0.753       0.78150\n",
       "37        39        0.752       0.78375\n",
       "19        21        0.748       0.78675\n",
       "18        20        0.747       0.78600\n",
       "16        18        0.745       0.77950\n",
       "17        19        0.744       0.78875\n",
       "15        17        0.742       0.78375\n",
       "10        12        0.738       0.78650\n",
       "11        13        0.734       0.79225\n",
       "14        16        0.733       0.78000\n",
       "13        15        0.732       0.78750\n",
       "12        14        0.731       0.78875\n",
       "9         11        0.730       0.79025\n",
       "8         10        0.727       0.78800\n",
       "7          9        0.723       0.79675\n",
       "5          7        0.718       0.80575\n",
       "6          8        0.712       0.78825\n",
       "4          6        0.704       0.79775\n",
       "3          5        0.695       0.82350\n",
       "2          4        0.689       0.80975\n",
       "1          3        0.683       0.85225\n",
       "0          2        0.643       0.82225"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_values_frame=Main_frame.sort_values(ascending=False,by=\"Test Scores\")\n",
    "k_values_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172ed4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372fb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "217ec8bc",
   "metadata": {},
   "source": [
    "### SVM Hyperparameter Optimization\n",
    "\n",
    "* **Kernels**: The main function of the kernel is to take low dimensional input space and transform it into a higher-dimensional space. It is mostly useful in non-linear separation problem.\n",
    "\n",
    "* **C (Regularisation)**: C is the penalty parameter, inverse of regularization strength; must be a positive float. It represents misclassification or error term. The misclassification or error term tells the SVM optimisation how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term.\n",
    "\n",
    "* **Gamma**: It defines how far influences the calculation of plausible line of separation. when gamma is higher, nearby points will have high influence; low gamma means far away points also be considered to get the decision boundary.\n",
    "\n",
    "* Tunning Hyperparemeters for SVM is handled with using GridSearch method of Scikit Learn Library. \n",
    "\n",
    "* To work faster, less data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b326d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X = data['review'].iloc[:5000]\n",
    "y = data['sentiment'].iloc[:5000]\n",
    "tfidf = TfidfVectorizer(max_features=300)\n",
    "X = tfidf.fit_transform(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "43f9f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "757b05b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   3.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   2.7s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   2.7s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   2.8s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   2.8s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   2.8s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   2.9s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   2.8s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   2.8s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   2.8s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   2.3s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   2.5s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   2.4s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   2.5s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   2.5s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   2.9s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   2.9s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   2.9s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   2.8s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   2.9s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   2.9s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   2.9s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   2.9s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   2.9s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   2.7s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   2.9s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   3.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   2.8s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   2.9s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   2.8s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   2.9s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   2.9s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   2.8s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   2.8s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   2.7s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   2.7s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   2.7s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   2.8s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   3.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   3.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   3.2s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   3.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   3.2s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   3.1s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   3.1s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   3.1s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   3.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   3.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   3.1s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   3.1s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   3.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   3.1s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   3.2s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   3.0s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   2.2s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   2.2s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   2.2s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   2.3s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   2.3s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   3.1s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   3.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   3.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   3.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   3.0s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   1.9s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   1.8s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   1.8s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   1.8s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   1.7s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   2.1s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   2.2s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   2.3s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   2.3s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   2.3s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   3.1s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   3.0s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   2.9s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   3.0s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   2.9s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   2.5s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   2.4s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   2.5s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   2.6s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   2.6s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   3.1s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   3.2s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   3.1s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   3.1s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   3.2s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   3.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   3.2s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   3.2s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   3.1s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   3.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   3.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   2.9s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   2.9s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   3.1s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   3.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   3.1s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   3.2s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   3.2s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   3.2s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   3.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   3.1s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   2.9s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   2.8s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   2.9s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   3.0s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   3.0s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   3.0s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   2.9s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   2.9s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   1.7s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   1.7s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   1.8s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   1.7s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   1.7s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   1.7s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   1.8s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   2.9s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   2.8s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   2.8s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   2.7s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   2.7s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   1.5s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   2.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   1.9s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   2.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   2.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   2.0s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   2.7s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   2.7s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   2.7s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   2.7s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   2.7s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   2.3s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   2.2s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   2.3s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   2.3s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   2.3s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   2.9s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   2.9s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   2.9s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   2.8s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   2.7s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   2.7s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   2.8s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   2.8s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   2.7s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   2.9s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   3.0s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   3.1s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   2.9s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   2.9s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   2.9s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   3.1s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   2.8s\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   3.0s\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   2.9s\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   2.9s\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   2.8s\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   2.9s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   1.5s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   2.7s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   2.8s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   2.6s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   2.8s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   2.3s\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   2.9s\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   2.8s\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   2.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   2.9s\n",
      "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   2.8s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   1.9s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   2.0s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   1.8s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   1.9s\n",
      "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   1.9s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   1.7s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   1.7s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   1.8s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   1.7s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   1.7s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   3.0s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   2.9s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   2.9s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   2.9s\n",
      "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   2.9s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   1.7s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   1.6s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   1.7s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   1.7s\n",
      "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   1.7s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   2.1s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   2.1s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   2.1s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   2.1s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   2.2s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   2.9s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   2.9s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   2.9s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   2.9s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   2.9s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   2.5s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   2.3s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   2.4s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   2.6s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   2.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001],\n",
       "                         'kernel': ['rbf', 'poly', 'sigmoid']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.svm import SVC\n",
    "grid = GridSearchCV(clf,parameters,refit=True,verbose=2)\n",
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a5a5fbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, gamma=1)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb05a5",
   "metadata": {},
   "source": [
    "### Results After HyperParameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0d7b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000) # for default Unable to allocate 77.6 GiB for an array with shape (50000, 208350) and data type float64\n",
    "reviews = list(data['review'])                  # so set max_feature \n",
    "labels = data['sentiment']\n",
    "\n",
    "tfidf_reviews = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e2a2e938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the array: (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_array = tfidf_reviews.toarray()\n",
    "print(\"Shape of the array:\",tfidf_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0dc8949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "NUM_COMPONENTS = 300\n",
    "pca = PCA(NUM_COMPONENTS)\n",
    "reduced = pca.fit_transform(tfidf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d8e3f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(reduced, labels, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1acd24d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.871, 0.8744)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression(max_iter=200,C=10)\n",
    "logistic_model.fit(x_train, y_train)\n",
    "logistic_model.score(x_test,y_test),logistic_model.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b1b79922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7503, 0.787425)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=25)\n",
    "knn.fit(x_train,y_train)\n",
    "#y_pred=knn.predict(x_test)\n",
    "#confusion_matrix(y_test,y_pred)\n",
    "knn.score(x_test,y_test), knn.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a555d312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8738, 0.890675)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "clf = svm.SVC(C=1,gamma=1)\n",
    "clf.fit(x_train, y_train)\n",
    "#y_pred = clf.predict(x_test)\n",
    "#confusion_matrix(y_test,y_pred)\n",
    "clf.score(x_test,y_test), clf.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec883071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd88a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d008666f",
   "metadata": {},
   "source": [
    "### Logistic Regression With Hand Coded Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6188b",
   "metadata": {},
   "source": [
    "### What the Logistic Model will do step by step?\n",
    "\n",
    "#####  PART 1\n",
    "\n",
    "* 1)First initialize parameters, weights and bias.\n",
    "* 2)Do matrix multiplication with x_train, meaning multiply each feature with an initialized weight and add bias term\n",
    "\n",
    "* 3) Put all matrix multiplication values ( values should be equal to number of review so each review has an valu of matrix multiplication.) into sigmoid function. Sigmoid function gives probablistic value. If values > 0.5 then it says positive, lower than < 0.5 says negative.\n",
    "\n",
    "* 4) Compare with true y_train values and find the cost using loss function.\n",
    "\n",
    "##### PART 2 \n",
    "\n",
    "* 5) Update weight and bias so that the costs will decrease. \n",
    "\n",
    "* 6) Updating parameters part need so much work, defining loss function, finding derivative, determining learning rate, iterations etc..\n",
    "\n",
    "* 7) Do this 5 steps with a specified number of iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff217fa",
   "metadata": {},
   "source": [
    "#### PART 1 is the Forward Propagation in general.\n",
    "\n",
    "* z = (w.T)x_train + b => in this equation we know x that is reivew array, we know w (weights) and b (bias. \n",
    "* Then we put z into sigmoid function that returns y_head(probability). \n",
    "* Then we calculate loss(error) function. \n",
    "* Cost function is summation of all loss(error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1af39cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['review'].iloc[:3000]\n",
    "y = data['sentiment'].iloc[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "28717d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "048b9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    " \n",
    "vectorizer.fit(X)\n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1e176704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=y.to_numpy()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f165ad07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>36509</th>\n",
       "      <th>36510</th>\n",
       "      <th>36511</th>\n",
       "      <th>36512</th>\n",
       "      <th>36513</th>\n",
       "      <th>36514</th>\n",
       "      <th>36515</th>\n",
       "      <th>36516</th>\n",
       "      <th>36517</th>\n",
       "      <th>36518</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 36519 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5      6      7      8      9      \\\n",
       "0         0      0      0      0      0      0      0      0      0      0   \n",
       "1         0      0      0      0      0      0      0      0      0      0   \n",
       "2         0      0      0      0      0      0      0      0      0      0   \n",
       "3         0      0      0      0      0      0      0      0      0      0   \n",
       "4         0      0      0      0      0      0      0      0      0      0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2995      0      0      0      0      0      0      0      0      0      0   \n",
       "2996      0      0      0      0      0      0      0      0      0      0   \n",
       "2997      0      0      0      0      0      0      0      0      0      0   \n",
       "2998      0      0      0      0      0      0      0      0      0      0   \n",
       "2999      0      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "      ...  36509  36510  36511  36512  36513  36514  36515  36516  36517  \\\n",
       "0     ...      0      0      0      0      0      0      0      0      0   \n",
       "1     ...      0      0      0      0      0      0      0      0      0   \n",
       "2     ...      0      0      0      0      0      0      0      0      0   \n",
       "3     ...      0      0      0      0      0      0      0      0      0   \n",
       "4     ...      0      0      0      0      0      0      0      0      0   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2995  ...      0      0      0      0      0      0      0      0      0   \n",
       "2996  ...      0      0      0      0      0      0      0      0      0   \n",
       "2997  ...      0      0      0      0      0      0      0      0      0   \n",
       "2998  ...      0      0      0      0      0      0      0      0      0   \n",
       "2999  ...      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "      36518  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "2995      0  \n",
       "2996      0  \n",
       "2997      0  \n",
       "2998      0  \n",
       "2999      0  \n",
       "\n",
       "[3000 rows x 36519 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_frame=pd.DataFrame.sparse.from_spmatrix(X)\n",
    "X_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d994d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_frame.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c3174d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e4dd842a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 36519)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "26f5f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f7e5cad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2400), (1, 600), (36519, 2400), (36519, 600))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to be with same dimensions in the following functions.\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "y_train.shape,y_test.shape,x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5591f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592373d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556183b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5ff35d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight_and_bias(dimension):\n",
    "    #dimension is the number of features. We need the same number of weights as the features are.\n",
    "    w = np.full((dimension,1),0.01)\n",
    "    b=0.0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "081ac97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    y_head = 1/(1+np.exp(-z))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f1ca1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cost(y_head,y_train):\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n",
    "    return cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9b614a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(w,b,x_train,y_train):\n",
    "    z = np.dot(w.T,x_train)+b\n",
    "    y_head = sigmoid(z)\n",
    "    cost = loss_cost(y_head,y_train)\n",
    "    return cost,y_head,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4aac7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients_after_forward(w,b,x_train,y_train):# BACKWARD PROPAGATION\n",
    "    cost,y_head,z = forward_propagate(w,b,x_train,y_train)\n",
    "    \n",
    "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n",
    "    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4069edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_wegihts_bias(w,b,x_train,y_train,learning_rate):\n",
    "    cost,gradients=calculate_gradients_after_forward(w,b,x_train,y_train)\n",
    "    # lets update\n",
    "    w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "    b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "    return w,b,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e7bb9c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_recursively(w,b,x_train,y_train,learning_rate,num_of_iteration):\n",
    "    cost_list=[]\n",
    "    cost_list2=[]\n",
    "    index = []\n",
    "    for i in range(num_of_iteration):\n",
    "        w,b,cost = update_wegihts_bias(w,b,x_train,y_train,learning_rate)\n",
    "        cost_list.append(cost)\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    #plt.xlabel(\"Number of Iteration\")\n",
    "    #plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters,cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "13304601",
   "metadata": {},
   "outputs": [],
   "source": [
    " # prediction\n",
    "def predict(w,b,x_test):\n",
    "    \n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "   \n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e9b2bac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.886342\n",
      "Cost after iteration 10: 0.619592\n",
      "Cost after iteration 20: 0.569981\n",
      "Cost after iteration 30: 0.533491\n",
      "Cost after iteration 40: 0.504976\n",
      "Cost after iteration 50: 0.481695\n",
      "Cost after iteration 60: 0.462077\n",
      "Cost after iteration 70: 0.445153\n",
      "Cost after iteration 80: 0.430288\n",
      "Cost after iteration 90: 0.417045\n",
      "Cost after iteration 100: 0.405111\n",
      "Cost after iteration 110: 0.394258\n",
      "Cost after iteration 120: 0.384309\n",
      "Cost after iteration 130: 0.375129\n",
      "Cost after iteration 140: 0.366611\n",
      "Cost after iteration 150: 0.358670\n",
      "Cost after iteration 160: 0.351235\n",
      "Cost after iteration 170: 0.344248\n",
      "Cost after iteration 180: 0.337661\n",
      "Cost after iteration 190: 0.331433\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEBCAYAAABojF4hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAl5ElEQVR4nO3deXxddZ3/8dcn+75vbdKmaZt0o7TQUISirEJRBGFgBHVA1MEqqIwzCvMbGZ2d34yO409ABhkWxwVZFIrsO7ZQutPSfW/TdEmaNGmSttm+vz/OabgNSXOSJr3Jve/n43Efufec87nnyyV593u/53vOMeccIiIy8sWEuwEiIjI4FOgiIhFCgS4iEiEU6CIiEUKBLiISIeLCteO8vDw3bty4cO1eRGREWrZsWa1zLr+ndWEL9HHjxrF06dJw7V5EZEQysx29rQs05GJmc81sg5ltNrM7e1ifbWZ/MLNVZrbYzE47mQaLiEj/9RnoZhYL3AtcDkwFbjCzqd02+z/ASufc6cCNwE8Hu6EiInJiQXros4HNzrmtzrlW4DHgqm7bTAVeA3DOrQfGmVnhoLZUREROKEigFwO7Ql5X+ctCvQ9cA2Bms4FSoKT7G5nZLWa21MyW1tTUDKzFIiLSoyCBbj0s634BmLuBbDNbCXwTWAG0f6TIuQecc5XOucr8/B4P0oqIyAAFmeVSBYwJeV0CVIdu4JxrBG4GMDMDtvkPERE5RYL00JcA5WZWZmYJwPXA/NANzCzLXwfwVeBtP+RFROQU6TPQnXPtwG3AS8A64HHn3Bozm2dm8/zNpgBrzGw93myYbw9Vg5fvrOfW3yznYEvrUO1CRGRECnRikXPueeD5bsvuD3n+LlA+uE3rWePhNp5btYcbP1bK2eNzT8UuRURGhBF3LZeKwnQANu5vCnNLRESGlxEX6KMyk0hPjGPTvkPhboqIyLAy4gLdzCgvTGOjAl1E5DgjLtDBG3bZtE9DLiIioUZkoJcXpnOguZXapqPhboqIyLAxIgO9ojANQMMuIiIhRmigezNdNOwiIvKhERnoBemJZCTFqYcuIhJiRAa6mTGpSAdGRURCjchAB+/A6Mb9h3Cu+4UfRUSi04gN9IqCNA62tFGjmS4iIsBIDvRjlwDYq2EXEREYwYFefizQdWBURAQYwYGel5ZAdko8m/Yr0EVEYAQHundNl3Q2aqaLiAgwggMdYFJhOhv3aaaLiAiM8ECvKEzj0JF29jVqpouIyIgO9GMHRjfowKiIyMgO9A+v6aJAFxEZ0YGek5pAXlqCpi6KiDDCAx2gvEAzXUREIAICfVJROpv3N2mmi4hEvREf6OWFaTQdbae64Ui4myIiElYjPtA/vKaLxtFFJLqN/EAv0DVdREQgAgI9MyWegvREHRgVkag34gMdvGEXXaRLRKJdoEA3s7lmtsHMNpvZnT2szzSzZ83sfTNbY2Y3D35Te1dR6N2OrrNTM11EJHr1GehmFgvcC1wOTAVuMLOp3Ta7FVjrnJsBXAD82MwSBrmtvaooTONwWwe7Dx4+VbsUERl2gvTQZwObnXNbnXOtwGPAVd22cUC6mRmQBtQB7YPa0hPouqaLZrqISBQLEujFwK6Q11X+slD3AFOAamA18G3nXGf3NzKzW8xsqZktrampGWCTP6q8MA2AjRpHF5EoFiTQrYdl3QerLwNWAqOBmcA9ZpbxkSLnHnDOVTrnKvPz8/vZ1N5lJMUzKjOJTZrpIiJRLEigVwFjQl6X4PXEQ90M/N55NgPbgMmD08Rgyv2bXYiIRKsggb4EKDezMv9A5/XA/G7b7AQuBjCzQmASsHUwG9qXioI0Nu9vokMzXUQkSvUZ6M65duA24CVgHfC4c26Nmc0zs3n+Zv8EnGtmq4HXgDucc7VD1eieVBSlc7S9k111LadytyIiw0ZckI2cc88Dz3dbdn/I82rg0sFtWv9UhNy9aFxeajibIiISFhFxpihAeYE300V3LxKRaBUxgZ6aGEdxVrKu6SIiUStiAh28M0Y100VEolWEBXo6W2uaae/4yDlNIiIRL+ICvbWjkx2a6SIiUSjiAh109yIRiU4RFegTC9IwQwdGRSQqRVSgJyfEMiY7RRfpEpGoFFGBDt5MF81FF5FoFHGBXl6YzrbaZto000VEokzEBfqkwnTaOhzba5vD3RQRkVMq4gK962YXOjAqIlEm4gJ9Qn4aMeZdpEtEJJpEXKAnxcdSmpuqA6MiEnUiLtDBu/KirukiItEmIgO9ojCd7QdaONreEe6miIicMpEZ6EXpdHQ6tmmmi4hEkcgMdM10EZEoFJGBXpaXSmyM6SJdIhJVIjLQE+NiGZebogOjIhJVIjLQwTswumm/hlxEJHpEbKCXF6az40AzR9o000VEokPEBnpFYRqdDrbUqJcuItEhYgN9kn/3ok2a6SIiUSJiA31cXirxsaZruohI1IjYQI+PjaEsT9d0EZHoESjQzWyumW0ws81mdmcP679rZiv9xwdm1mFmOYPf3P4pL0zXyUUiEjX6DHQziwXuBS4HpgI3mNnU0G2cc//hnJvpnJsJ/C3wlnOubgja2y8VBensqm/hcKtmuohI5AvSQ58NbHbObXXOtQKPAVedYPsbgN8ORuNOVkVhGs7BZs1HF5EoECTQi4FdIa+r/GUfYWYpwFzgqV7W32JmS81saU1NTX/b2m8VRd5MF50xKiLRIEigWw/LXC/bfgZY2Ntwi3PuAedcpXOuMj8/P2gbB6w0J4WE2BgFuohEhSCBXgWMCXldAlT3su31DJPhFoC42BjG56cq0EUkKgQJ9CVAuZmVmVkCXmjP776RmWUC5wPPDG4TT06FZrqISJToM9Cdc+3AbcBLwDrgcefcGjObZ2bzQja9GnjZOTes7ipRUZjG7oOHaT7aHu6miIgMqbggGznnngee77bs/m6vHwEeGayGDZbyY5cA2N/EzDFZ4W2MiMgQitgzRY85dk0XjaOLSKSL+EAfk5NCYlyM7l4kIhEv4gM9NsaYWJDGRp1cJCIRLuIDHfy7F2nIRUQiXFQEenlhGnsajtB4pC3cTRERGTJREegVBbrZhYhEvugI9K67F2nYRUQiV1QEekl2Msnxsbp7kYhEtKgI9JgYo7wwTUMuIhLRoiLQAcoL0nVykYhEtKgJ9IrCNPYfOkpDi2a6iEhkiqJA9y8BsF+9dBGJTFET6OWFaYCu6SIikStqAr04K5nUhFgdGBWRiBU1gW5mlBems0EX6RKRCBU1gQ7egdFNGkMXkQgVZYGeTm1TK3XNreFuiojIoIuqQC/XzS5EJIJFVaBX+DNddE0XEYlEURXoRRlJpCfFsVEzXUQkAkVVoJsZFYXpukiXiESkqAp08Ge67DuEcy7cTRERGVRRF+jlBenUt7Sxr/FouJsiIjKooi7Qz52YS1yMcfvvVnCkrSPczRERGTRRF+iTizL48Z/PYNHWOv7qdyvp6NTQi4hEhqgLdICrZhZz1xVTeeGDvfz9Mx9oPF1EIkJcuBsQLl85r4yaQ0e5/60tFKQn8e1LysPdJBGRkxKoh25mc81sg5ltNrM7e9nmAjNbaWZrzOytwW3m0Lhj7iT+7MwSfvLqRn793o5wN0dE5KT02UM3s1jgXuCTQBWwxMzmO+fWhmyTBdwHzHXO7TSzgiFq76AyM+7+s+nUt7Ry19MfkJuawNzTRoW7WSIiAxKkhz4b2Oyc2+qcawUeA67qts3ngd8753YCOOf2D24zh058bAz3fv5MZozJ4luPrWTR1gPhbpKIyIAECfRiYFfI6yp/WagKINvM3jSzZWZ2Y09vZGa3mNlSM1taU1MzsBYPgeSEWB666SzG5qTwl48uZW11Y7ibJCLSb0EC3XpY1n1aSBwwC/g0cBlwl5lVfKTIuQecc5XOucr8/Px+N3YoZacm8MsvzyYtKY6bHl7MrrqWcDdJRKRfggR6FTAm5HUJUN3DNi8655qdc7XA28CMwWniqTM6K5lffnk2re2d3PjQYg406WxSERk5ggT6EqDczMrMLAG4HpjfbZtngI+bWZyZpQBnA+sGt6mnRnlhOg996Sz2NBzm5keW0Hy0PdxNEhEJpM9Ad861A7cBL+GF9OPOuTVmNs/M5vnbrANeBFYBi4EHnXMfDF2zh9as0mzu/fyZrKluZN6vltHa3hnuJomI9MnCdZZkZWWlW7p0aVj2HdQTS3fx3SdXcdXM0fzkz2cSE9PT4QQRkVPHzJY55yp7Whe1Z4oGcV3lGGqbWvm/L64nNzWRu66YgplCXUSGJwV6H+adP56aQ0d5aOE2CjISmXf+hHA3SUSkRwr0PpgZ3//0FGqbjnL3C+vJTU3gusoxfReKiJxiCvQAYmKMH103g/qWVu78/Wpy0xK4aHJhuJslInKcqLx87kAkxMXw8y/OYtroDL7x6+W8tGZvuJskInIcBXo/pCXG8fCXzmJSYTpf+99l/PTVTXTqBhkiMkwo0PspNy2R333tHK45s5ifvLqRb/x6uU4+EpFhQYE+AEnxsfz4uhncdcVUXl67l2vue4edB3TtFxEJLwX6AJkZXzmvjEe/PJu9jUe48t4FLNxcG+5miUgUU6CfpI+X5zP/tjkUpCdy40OL+Z8F23SPUhEJCwX6ICjNTeX335jDJVMK+Kc/ruVvnljFkbaOcDdLRKKMAn2QpCXG8fMvzOL2S8p5ankVn3tgEfsaj4S7WSISRRTogygmxrj9kgr++y9msXnfIT7zswUs31kf7maJSJRQoA+By6YV8ftvzCEpPpbr/3sRjy/d1XeRiMhJUqAPkUlF6cy/bQ6zy3L43pOr+OH8NbR16LrqIjJ0FOhDKCslgUduPouvnFfGI+9s58b/WUxdc2u4myUiEUqBPsTiYmO464qp/Pi6GSzbWc+V9yxg3Z7GcDdLRCKQAv0U+bNZJTz+tXNo6+jkmvve4eGF2+jQdWBEZBAp0E+hmWOyePa28zirLId/eHYt1/z8HfXWRWTQKNBPsYKMJB69+Sx+ev1Mqupa+MzPFvDvL67XiUgictIU6GFgZlw1s5hXv3M+V59RzH1vbuGy/3pb14IRkZOiQA+j7NQE/uO6Gfzmq2djwBcefI+/eeJ96jUTRkQGQIE+DJw7MY8Xb/8Et144gadX7Obi/3yLp1fs1kW+RKRfFOjDRFJ8LN+9bDLPfvM8xuakcPvvVnLTw0vYVafrrItIMAr0YWbKqAye+vq5/MOV01i2vY5Lf/I2v3h7K+06y1RE+qBAH4ZiY4ybzh3HK985nzkT8/iX59dx1b0LWV3VEO6micgwpkAfxkZnJfOLG2fx8y+cyf5DR7nq3gX88x/X0tKqe5iKyEcFCnQzm2tmG8xss5nd2cP6C8yswcxW+o+/H/ymRicz4/Lpo3j1O+dz/eyxPLhgG5/8z7d5esVunWkqIsfpM9DNLBa4F7gcmArcYGZTe9j0T865mf7jHwe5nVEvMzmef716Ok/MO4eM5Hhu/91KLv/p27z4wR7NhhERIFgPfTaw2Tm31TnXCjwGXDW0zZLenDUuh+e+eR73fP4M2jsd8361nCvvWcgbG/Yr2EWiXJBALwZC79BQ5S/r7hwze9/MXjCzaT29kZndYmZLzWxpTU3NAJor4N0Z6YrTR/Py7Z/gR9fNoL6llZsfXsJ197/Lu1sOhLt5IhImQQLdeljWvSu4HCh1zs0AfgY83dMbOececM5VOucq8/Pz+9VQ+ai42BiunVXC6399Af/82dPYVd/CDb9YxBcffI8VuvWdSNQJEuhVwJiQ1yVAdegGzrlG51yT//x5IN7M8gatlXJCCXExfPFjpbz13Qv5/qensG5PI1ff9w5ffXQJa6o11VEkWgQJ9CVAuZmVmVkCcD0wP3QDMysyM/Ofz/bfV9/9T7Gk+Fi++vHxvP29C/nuZZNYvK2OT/+/Bdz6m+Vs3t8U7uaJyBCL62sD51y7md0GvATEAg8559aY2Tx//f3AtcDXzawdOAxc73SELmxSE+O49cKJfPFjpTz4p608tGAbL6zew9VnlPDti8sZm5sS7iaKyBCwcOVuZWWlW7p0aVj2HW0ONB3l/re28Mt3d9DR6fjcWWO45RPjKc1NDXfTRKSfzGyZc66yx3UK9Oixt+EI976xmceW7KS903HJlEJunjOOc8bn4o+Yicgwp0CX4+xrPMKvFu3g1+/tpK65lclF6Xz5vDKunDGapPjYcDdPRE5AgS49OtLWwfyV1Ty0cBvr9x4iNzWBL5w9li9+rJSCjKRwN09EeqBAlxNyzvHulgM8tHAbr63fT5x/4tKX55QxvSQz3M0TkRAnCvQ+Z7lI5DMzzp2Yx7kT89he28wj72zniaW7+MOK3Zw1Lpub55Rx6dRC4mJ1cU6R4Uw9dOlR45E2Hl+yi0ff3c6uusMUZyVz07mlfK5yLJkp8eFunkjU0pCLDFhHp+PVdft4aME23ttWR3J8LNfOKuH62WOYNlrDMSKnmgJdBsWa6gYeXrid+Surae3oZOqoDK6dVcJVM0eTm5YY7uaJRAUFugyq+uZWnl1VzRNLq1i9u4H4WOOiyQVcO2sMF0zKJ15j7SJDRoEuQ2b93kaeWlbFH1bspraplby0BD47s5hrK0uYXJQR7uaJRBwFugy5to5O3tpQw5PLqnht/T7aOhzTizO5dlYJV84YTXZqQribKBIRFOhyStU1t/LMyt08uayKNdWNxMcal0wp5LrKEj5Rnq/pjyInQYEuYbO2upEnl1Xx9Mrd1DW3kp+eyNVnFPOZ00dzWnGGriEj0k8KdAm71vZO3tywnyeWVfHG+v20dzrG5qRw+fQiPnXaKE4vyVS4iwSgQJdhpb65lVfW7uO51XtYuLmW9k5HcVYyn5pexKemj2LmmCyFu0gvFOgybB1s8cL9+dV7WLC5lrYOx+jMJC6fPopPTR/FGWOyiIlRuIsco0CXEaGhpY1X13nh/qdNtbR2dDIqM4m5p3k991ljsxXuEvUU6DLiNB5p47V1+3hu1V7e3lRDa3snhRmJXH7aKC4/rYjKcTnEKtwlCinQZUQ7dKSN19fv57lVe3hzoxfuWSnxXFCRz4WTCzi/Ip+sFM1zl+igQJeI0XS0nTc37Of1dft5c2MNdc2txMYYs8Zmc+HkAi6eUkB5QZoOqkrEUqBLROrodKzcdZA31u/n9fX7WbunEYDirGQumlzARVMKOGd8rm6rJxFFgS5RYU/DYd5YX8Pr6/ezcHMth9s6SIqP4byJeVw4uYCLJhcwKjM53M0UOSkKdIk6R9o6WLT1AK/7vfeq+sMATBmVwUWT8/lEeT5njM0mIU6XIZCRRYEuUc05x6b9TV3hvmxHPR2djuT4WGaX5XDexDzmTMxjclG6pkXKsKdAFwnRcLiN97YeYOHmWhZsrmVLTTMAuakJnDsxj/Mm5jJnYh4l2SlhbqnIR+km0SIhMpPjuXRaEZdOKwK8sfeFmz8M+GffrwZgXG4Kcybmcd7EPM6ZkKupkTLsBeqhm9lc4KdALPCgc+7uXrY7C1gEfM459+SJ3lM9dBmOjg3PLNhUy8LNtSzaeoDm1g7MYHpxJnMm5jFnQh6zSrNJTtDsGTn1TmrIxcxigY3AJ4EqYAlwg3NubQ/bvQIcAR5SoEskaOvo5P1dB1mw2Qv4FTsP0t7piI81phdnMrssl7PLcpg1LpuMpPhwN1eiwMkG+jnAD51zl/mv/xbAOfdv3ba7HWgDzgL+qECXSNR0tJ0l2+p4b1sdi7cdYPXuBto6HGYwpSiD2WU5nF2Ww1llOeTpxtkyBE52DL0Y2BXyugo4u9sOioGrgYvwAr23htwC3AIwduzYALsWGV7SEuO4cHIBF04uAOBwawcrdtWzeFsdi7fV8diSnTzyznYAxuencnZZDrPLcphdlktxlubAy9AKEug9zePq3q3/L+AO51zHiU65ds49ADwAXg89YBtFhq3khFjOnZDHuRPyAO9GHh9UN3QF/B9X7eG3i73+UHFWsh/uOcwqzWZifpqmScqgChLoVcCYkNclQHW3bSqBx/wwzwM+ZWbtzrmnB6ORIiNFQlwMZ47N5syx2cw7fwIdnY4New+xeNsBFm+v40+bavnDit0ApCfGMXNsFmeMzebMsVmcMSabzBSNw8vABRlDj8M7KHoxsBvvoOjnnXNretn+ETSGLtIj5xzbaptZvvMgK3bWs3znQTbsbaTT/zOckJ/KmWOzvZAvzaK8IF2XCZbjnNQYunOu3cxuA17Cm7b4kHNujZnN89ffP6itFYlgZsb4/DTG56dx7awSwDvQuqrqICt2HmT5jnpeW+/dexW8MfsZYzK7ev0zx2SRnar58NIznSkqMsw459hxoIXlO+u9kN9Zz/q9h+jwu/Hj81KZMSaL6cWZnF6SydTRGaQk6BzBaKEzRUVGEDNjXF4q4/JSueZMrxff0trOqqqGrpB/d8uBrrH4GIOKwvSugJ9eksWUUekkxunEp2ijQBcZAVIS4vjY+Fw+Nj63a9m+xiOsrmpg1e4GVlUd5PWQoZr4WGNSUTrTi7O8kC/OZFJROvGxurpkJNOQi0iEcM5R3XCE1VUHeb+qwQv7qoM0HmkHvBk4U0dlcHpJJqeN9oZqygvT1JMfYXS1RZEo5ZxjZ12LH/AHWVXVwAe7G2hu7QAgLsaYWJDGND/gp43OYMqoDDKTNX1yuFKgi0iXzk7HjroW1lQ3sLa6kTXVjazd00jNoaNd24zJSWbqqAymjc5k2ugMpo7OoCgjSfdqHQZ0UFREusTEGGV5qZTlpXLF6aO7lu8/dOS4gF9X3cjLa/dxrM+Xk5rgh3wGk0elM6kwgwkFqRqyGUYU6CICQEF6EgWTkrhgUkHXsqaj7WzY64e8H/YPL9xOa0cn4A3ZjM9PZVJRBpOL0plUmM6konRKspPVmw8DBbqI9CotMY5ZpTnMKs3pWtbW0cn22mbW7z3Ehr2HWL+3kRU767tuDALeZQ0qirxwn1yUzuSiDCYVpWtsfohpDF1EBsWhI21s3NfE+r2NftB7gd9wuK1rm1GZSUzye/ITC9Io93+mJapvGZTG0EVkyKUnxTOrNJtZpdldy5xz7Gs8yjo/5I8F/TtbDtDa3tm1XXFWshfwBWlUFKYzsTCNiQVpumlIPynQRWTImBlFmUkUZSZxYcjYfHtHJ7vqD7Np3yE27W/q+rlo6wGOhgR9UUYS5YVplBekU16YRkVhGhPz03VVyl4o0EXklIuLjemaaXPptA+Xd3Q6qupb2LSv6big/+3inRxu6+jaLj89kfF5qUwoSGNCfhoT8lOZkJ9GcVZyVF9jXoEuIsNGbIxRmptKaW4ql0wt7Fre2enYffAwm/YfYuO+JrbWNLGlppnnVu05bow+Mc77h6J70JflpZIaBeP0kf9fKCIjXkyMMSYnhTE5KVw0+cOgd85R19zKlppmP+S9oP9gdwMvrN7TdZ15gNGZSYz3Q74sL5Wy/DTG56UyOis5Yq45r0AXkRHLzMhNSyQ3LZHZZTnHrTva3sGOAy1s2f9h0G+paeKp5btpOtretV1CbAxjc1MYl5vK+PxUxuWmdg0HFWYkjqj59Ap0EYlIiXGxVBSmU1GYftxy5xw1TUfZVtPM9gPNbK1tZnttM9tqm3l7U81xs29SEmIpzU1lfF4q4/JSKMtLoyzPC/+c1IRhF/YKdBGJKmbmnRWbnsTZIZcjBm+svrrhMNtrW9hW28Q2/+faPY28uGZv101GwDvpqjQ3xX+kUpqT0tXTL8pICsvBWQW6iIgvJsYoyU6hJDuF88rzjlvX1tFJVf1httU2sb22hZ11LWw/0Mz6PYd4Ze0+2jo+DPuEuBjG5qRQmuOHfe6HYV+clUxC3NBcl16BLiISQHzIVMvuOjod1QcPd4X8zgPezx0HWnh36wFaWj+cchljcOuFE/nrSycNehsV6CIiJyk2ZBbOnInH9+yPjdl7Id/CzgPNnBFyNu1gUqCLiAyh0DH7ynE5fRecBN1gUEQkQijQRUQihAJdRCRCKNBFRCKEAl1EJEIo0EVEIoQCXUQkQijQRUQiRNhuEm1mNcCOAZbnAbUnsftorx8ObVC96lU/MKXOufwe1zjnRtwDWKp6fYaqV3201vf20JCLiEiEUKCLiESIkRroD6j+pIW7DapXveoHWdgOioqIyOAaqT10ERHpRoEuIhIhFOgiIhFiRNyxyMwmA1cBxYADqoH5zrl1YW2YiMgwMux76GZ2B/AYYMBiYIn//Ldmducp2H+mmd1tZuvN7ID/WOcvy1J9ZNeLjCTDfpaLmW0Epjnn2rotTwDWOOfKA7xHJvC3wGeBY6fM7geeAe52zh08Qe1LwOvAo865vf6yIuAm4BLn3Cf72LfqR3B9yPsYMJvjvyUudgH/gFSv+pOpD2okBPp64DLn3I5uy0uBl51zkwK8x4D/qM1sQ2/7ONE61UdGvb/dpcB9wCZgt7+4BJgIfMM597LqVT9U9f0yFNcTGMwHMBfYDLyANxn/AeBFf9ncgO+xYSDr/PUvA98DCkOWFQJ3AK8G2LfqR3C9v/06YFwPy8uAdapX/VDW9+cx7MfQnXMvAhXAPwAv4f2B/hCY5K8LYoeZfc/MCo8tMLNC88bnd/VR+zkgF3jLzOrNrA54E8gB/jzAvge7vt6vzw3T/kdq/ZtmVjeAevAmD1T1sHw3EK961Q9xfWAjYpaLc64TWHQSb/E54E68UCjwl+0D5gPX9bHvejN7GHgFWOScazq2zszm4n1b6Kv+KeBJ59wSM5uG961jnXOurq+GO+fq8XqTd4Ts93+dc9/rq7anejP7ON5Y3uog+8f7x/RfnXN3mFkK3ud4pr+uI0D9XwD3OOfu6HPLnjUDa4FXnHOvmtkXgHOBncChvor9z/8XeJcqHQO0AxuB3zrnGgK24SFgiZk9xocdgDHA9cD/RGn9WLy/q2jd/6msD2zYj6EPNTO72Tn38AnWfwu4Fe9r00zg2865Z/x1y51zZ/ZW62/zA+ByvH88X8EL07eAS4CXnHP/0kf9/B4WX4R3TADn3JV91C92zs32n3/V/295GrgUeNY5d3cf9WuAGc65djN7AC9gnwIu9pdf00d9g1+zBfgN8IRzLvB1oM3s13ifXTLQAKQCf/D3b865m/qo/xZwBfA28ClgJVAPXI03fvlmwHZMBa7EO6hleD2u+c65tQHrp/Dh1NuB1J/s/sPd/pFeH9bPP7DBHL8ZiQ9gZx/rVwNp/vNxwFK8UAdYEeD9VwOxQArQCGT4y5OBVQHqlwO/Ai4Azvd/7vGfnx+gfkXI8yVAvv88Fa+X3lf9utC2dFu3Msj+8abHXorXG6nB+1ZzE5AeoH6V/zMO71tVrP/aAn5+q0NqUoA3/edjg/z/06PXz7UgzPvPDfdnMBwfw34MfTCY2apeHqvxDpCdSKzzh1mcc9vxAvVyM/tPvFDpS7tzrsM51wJscc41+u91GOgMUF8JLAP+DmhwXo/ysHPuLefcWwHqY8ws28xy8Xq0Nf7+m/GGH/rygZnd7D9/38wqAcysAmjrvayLc851Oudeds59BRiNd8R/LrA1YPsTgHS8QM70lycSfPzx2NBiov8+OOd2Bq23IZzLbmYvBNgmw8z+zcz+18xu6LbuvgD1RWb2czO718xyzeyH/u//42Y2KkB9TvcHsNj/vcoJUD835HmmmT3o7/83oce1TlB/t5nl+c9nmdlWYJGZ7TCz8wPULzez75vZ+L627aW+0szeMLNfmdkYM3vFzA6a2RIzOyNAfZqZ/aOZrTGzBjOrMbNFZvalgbTnhML9L8qpeOD17GYCpd0e44DqPmpfB2Z2WxYH/BLoCLDv94AU/3lMyPJMuvV4+3ifEuAJ4B76+FbRrW47XnBu838W+cvTCNbDzgQewRsyeQ8vxLfiDRvNCFC/4gTrkgPU/5W/vx3At4DXgF/g9bx/EKD+28AqvNlR64Gb/eX5wNsBP8OX8I5BFIUsK8I7nvBKgPoze3nMAvYEqH8KuBvvPIr5/utEf12fv0N434i+6bd3lf/fMtZf9kyA+k7/9yf00XbsdypA/fKQ5w8C/+z//f0V8HSA+tUhz98AzvKfVxDgzj9+O3+Ed9xlsb/f0f34G1qMN2x6A94Y+LX+8ouBdwPUPwN8yf8b/g5wF1AOPIp3fCpQOwK1dTDfbLg+8L7qn9fLut/0UVsS+ofcbd2cAPtO7GV5HjB9AP8tnx6MXwK83m5ZP7ZPB2b4IVTYj7qKQWjr6GN/gEAWcC0wux/10/yayQPc/4CnvfrbdOB1DN7o4XE4QP3Kbq//DliIN3snSKCvCHm+80Tv3Uv93/j/KEwPWbatH5/f8t72F3D/64E4//mibuuCDBuG7v/jeN8Q9/qf/y0n+fmtCFD/frfXS/yfMcD6gfxO9vYYEbNcTpbzvur3tu7zfdT2NN3o2LqFAfZ9tJfltQzgJrHOueeA5/pb18P7tOD1XIJufwh4fwD72djfmh7eozrk+UHgyX7WrwHWnEQTdpjZ9/BOTNsH3rRXvF5XX9NewTug/jXn3KbuK8wsSH2imcU4b7YXzrl/MbMqvAO9aQHqQ4dWf9ltXWxfxc65H/kzNH7it/cHeGc7BlVgZt/BG6LMMDNzfqIR7PIj9wLPm9ndwItm9l/A7/F6yCv70Q6cc38C/mRm3wQ+iTdTpq+bTRwx7+SgTMCZ2Wedc0/7wz1BZno1m9l5zrkFZvYZoM5vS6eZBRm2DSwqAl3kJA142qvvh/QeXN8MUP8s3symV48tcM49amb7gJ8FqH/GzNKcc03Oue8fW2hmE4ENAeqPdWyu8wPpFbxveEH9Av/YBd4wQx5QY97Z2isD7Ptn/vGur+MNs8T5P58G/inA/j/SqXDOdeB96whyLss84N/xhp4uA75uZo/gzSP/y4D1D/rHnT4AvgxgZvl4/1gNmqiftihyMqyPaa+RWG9mycAE59wHI7H9kVT/kfdToIsMnJntdM6NVb3qw1HfnYZcRPpgZqt6W0Xf015Vr/qTqu8PBbpI3wrxxk7ruy034B3Vq36I6wNToIv07Y94Zwuv7L7CzN5UveqHuD4wjaGLiESIqDj1X0QkGijQRUQihAJdRCRCKNBFRCLE/wcxqqwnBHx9owAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 91.33333333333333 %\n",
      "test accuracy: 81.0 %\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n",
    "    \n",
    "    dimension =  x_train.shape[0]  # that is 19000\n",
    "    w,b = initialize_weight_and_bias(dimension) #initialize_weights_and_bias(dimension)\n",
    "    \n",
    "    parameters, cost_list = do_recursively(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n",
    "\n",
    "    \n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    \n",
    "logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.1, num_iterations = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90271d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40224064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a3a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc87ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3992e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bdaee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a1d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39656f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
